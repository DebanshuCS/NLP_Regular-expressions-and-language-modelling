{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Building a language model/ n-gram (Unigram, bigram, and trigram).**\n",
        "\n",
        "Step-1: Build a text corpus using any one or two of the English books given in the above link\n",
        "(https://www.gutenberg.org/). These books (text versions) will be used as text corpus. You\n",
        "compile one or more books according to their genre into a single text corpus.\n",
        "\n",
        "Step-2: Write a program to count the word frequencies (unigram), bigram, and trigram.\n",
        "\n",
        "Step -3: Prepare a bigram probability table showing probabilities of some frequent bigrams. You can also do Add-1 smoothing.\n",
        "Using these probabilities, calculate probabilities of some sentences(taking from the corpus and outside).\n",
        "\n",
        "Step-4: Using the above bigram and trigram probability models that you prepare, predict the next word (top 5 probable) given the previous n-gram for the sentence below.\n"
      ],
      "metadata": {
        "id": "I_fl-LT_YUQb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfhaHlQKYKjs"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.utils import to_categorical\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Activation, Dropout, Dense\n",
        "from keras.layers import Flatten, LSTM\n",
        "from keras.layers import GlobalMaxPooling1D\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, GRU, Embedding\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "metadata": {
        "id": "ye1nLIDycrSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import reuters\n",
        "from nltk import bigrams, trigrams\n",
        "from collections import Counter, defaultdict\n",
        "import nltk\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')\n",
        "\n",
        "#print(dict(model['today', 'the']))"
      ],
      "metadata": {
        "id": "QxLuUxafbGRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(text)"
      ],
      "metadata": {
        "id": "uKHHXQsUfFMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk, re, pprint, string\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "string.punctuation = string.punctuation +'“'+'”'+'-'+'’'+'‘'+'—'\n",
        "string.punctuation = string.punctuation.replace('.', '')\n",
        "#Add any of your text file and proceed. \n",
        "\n",
        "file = open('nlptext.txt', encoding = 'utf8').read()"
      ],
      "metadata": {
        "id": "LV432KyWfSVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocess data\n",
        "file_nl_removed = \"\"\n",
        "for line in file:\n",
        "  line_nl_removed = line.replace(\"\\n\", \" \")      #removes newlines\n",
        "  file_nl_removed += line_nl_removed\n",
        "file_p = \"\".join([char for char in file_nl_removed if char not in string.punctuation])   #removes all special characters"
      ],
      "metadata": {
        "id": "66N4denuflRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Now that we have removed special characters and newlines\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "IJWbCSchgCI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sents = nltk.sent_tokenize(file_p)\n",
        "print(\"The number of sentences is\", len(sents)) \n",
        "#prints the number of sentences\n",
        "words = nltk.word_tokenize(file_p)\n",
        "print(\"The number of tokens is\", len(words)) \n",
        "#prints the number of tokens\n",
        "average_tokens = round(len(words)/len(sents))\n",
        "print(\"The average number of tokens per sentence is\",\n",
        "average_tokens) \n",
        "#prints the average number of tokens per sentence\n",
        "unique_tokens = set(words)\n",
        "print(\"The number of unique tokens are\", len(unique_tokens)) \n",
        "#prints the number of unique tokens"
      ],
      "metadata": {
        "id": "D5WrMrGcf-gG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Building the N-gram Models\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "HaaMA-XtgakU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "unigram=[]\n",
        "bigram=[]\n",
        "trigram=[]\n",
        "fourgram=[]\n",
        "tokenized_text = []\n",
        "for sentence in sents:\n",
        "    sentence = sentence.lower()\n",
        "    sequence = word_tokenize(sentence) \n",
        "    for word in sequence:\n",
        "        if (word =='.'):\n",
        "            sequence.remove(word) \n",
        "        else:\n",
        "            unigram.append(word)\n",
        "    tokenized_text.append(sequence) \n",
        "    bigram.extend(list(ngrams(sequence, 2)))  \n",
        "#unigram, bigram, trigram, and fourgram models are created\n",
        "    trigram.extend(list(ngrams(sequence, 3)))\n",
        "    fourgram.extend(list(ngrams(sequence, 4)))\n",
        "def removal(x):     \n",
        "#removes ngrams containing only stopwords\n",
        "    y = []\n",
        "    for pair in x:\n",
        "        count = 0\n",
        "        for word in pair:\n",
        "            if word in stop_words:\n",
        "                count = count or 0\n",
        "            else:\n",
        "                count = count or 1\n",
        "        if (count==1):\n",
        "            y.append(pair)\n",
        "    return(y)\n",
        "bigram = removal(bigram)\n",
        "trigram = removal(trigram)             \n",
        "fourgram = removal(fourgram)\n",
        "freq_bi = nltk.FreqDist(bigram)\n",
        "freq_tri = nltk.FreqDist(trigram)\n",
        "freq_four = nltk.FreqDist(fourgram)\n",
        "print(\"Most common n-grams without stopword removal and without add-1 smoothing: \\n\")\n",
        "print (\"Most common bigrams: \", freq_bi.most_common(5))\n",
        "print (\"\\nMost common trigrams: \", freq_tri.most_common(5))\n",
        "print (\"\\nMost common fourgrams: \", freq_four.most_common(5))"
      ],
      "metadata": {
        "id": "W9AymebKgHWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#stopwords = code for downloading stop words through nltk\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "#prints top 10 unigrams, bigrams after removing stopwords\n",
        "print(\"Most common n-grams with stopword removal \\n\")\n",
        "unigram_sw_removed = [p for p in unigram if p not in stop_words]\n",
        "fdist = nltk.FreqDist(unigram_sw_removed)\n",
        "print(\"Most common unigrams: \", fdist.most_common(10))\n",
        "bigram_sw_removed = []\n",
        "bigram_sw_removed.extend(list(ngrams(unigram_sw_removed, 2)))\n",
        "fdist = nltk.FreqDist(bigram_sw_removed)\n",
        "print(\"\\nMost common bigrams: \", fdist.most_common(10))"
      ],
      "metadata": {
        "id": "Du-DiNVHgihL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Add-1 smoothening is performed here \n",
        "\n",
        "ngrams_all = {1:[], 2:[], 3:[], 4:[]}\n",
        "for i in range(4):\n",
        "    for each in tokenized_text:\n",
        "        for j in ngrams(each, i+1):\n",
        "            ngrams_all[i+1].append(j);\n",
        "ngrams_voc = {1:set([]), 2:set([]), 3:set([]), 4:set([])}\n",
        "for i in range(4):\n",
        "    for gram in ngrams_all[i+1]:\n",
        "        if gram not in ngrams_voc[i+1]:\n",
        "            ngrams_voc[i+1].add(gram)\n",
        "total_ngrams = {1:-1, 2:-1, 3:-1, 4:-1}\n",
        "total_voc = {1:-1, 2:-1, 3:-1, 4:-1}\n",
        "for i in range(4):\n",
        "    total_ngrams[i+1] = len(ngrams_all[i+1])\n",
        "    total_voc[i+1] = len(ngrams_voc[i+1])                       \n",
        "    \n",
        "ngrams_prob = {1:[], 2:[], 3:[], 4:[]}\n",
        "for i in range(4):\n",
        "    for ngram in ngrams_voc[i+1]:\n",
        "        tlist = [ngram]\n",
        "        tlist.append(ngrams_all[i+1].count(ngram))\n",
        "        ngrams_prob[i+1].append(tlist)\n",
        "    \n",
        "for i in range(4):\n",
        "    for ngram in ngrams_prob[i+1]:\n",
        "        ngram[-1] = (ngram[-1]+1)/(total_ngrams[i+1]+total_voc[i+1])             #add-1 smoothing"
      ],
      "metadata": {
        "id": "KsxdDaUbi8_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Most common n-grams without stopword removal and with add-1 smoothing: \\n\")\n",
        "for i in range(4):\n",
        "    ngrams_prob[i+1] = sorted(ngrams_prob[i+1], key = lambda x:x[1], reverse = True)\n",
        "    \n",
        "print (\"Most common unigrams: \", str(ngrams_prob[1][:10]))\n",
        "print (\"\\nMost common bigrams: \", str(ngrams_prob[2][:10]))\n",
        "print (\"\\nMost common trigrams: \", str(ngrams_prob[3][:10]))\n",
        "print (\"\\nMost common fourgrams: \", str(ngrams_prob[4][:10]))"
      ],
      "metadata": {
        "id": "NbxZEW0nkKBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Next word predictions for the strings using the probability models of bigrams, trigrams, and fourgrams\\n\")\n",
        "print(\"String 1 - after that alice said the-\\n\")\n",
        "print(\"Bigram model predictions: {}\\nTrigram model predictions: {}\\nFourgram model predictions: {}\\n\" .format(pred_1[1], pred_1[2], pred_1[3]))\n",
        "print(\"String 2 - alice felt so desperate that she was-\\n\")\n",
        "print(\"Bigram model predictions: {}\\nTrigram model predictions: {}\\nFourgram model predictions: {}\" .format(pred_2[1], pred_2[2], pred_2[3]))"
      ],
      "metadata": {
        "id": "LjAlWi6piTpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "str1 = 'Jane was as much gratified by this as her mother could be, though in a quieter way'\n",
        "str2 = 'we have had a most delightful evening, a most excellent ball. I wish you had been there'"
      ],
      "metadata": {
        "id": "U9QnhePrhhzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_1 = word_tokenize(str1)\n",
        "token_2 = word_tokenize(str2)\n",
        "ngram_1 = {1:[], 2:[], 3:[]}   #to store the n-grams formed  \n",
        "ngram_2 = {1:[], 2:[], 3:[]}\n",
        "for i in range(3):\n",
        "    ngram_1[i+1] = list(ngrams(token_1, i+1))[-1]\n",
        "    ngram_2[i+1] = list(ngrams(token_2, i+1))[-1]\n",
        "print(\"String 1: \", ngram_1,\"\\nString 2: \",ngram_2)"
      ],
      "metadata": {
        "id": "r9w6gc1JiL2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Add-1 smoothing is performed here.\n",
        "for i in range(4):\n",
        "    ngrams_prob[i+1] = sorted(ngrams_prob[i+1], key = lambda x:x[1], reverse = True)\n",
        "    \n",
        "pred_1 = {1:[], 2:[], 3:[]}\n",
        "for i in range(3):\n",
        "    count = 0\n",
        "    for each in ngrams_prob[i+2]:\n",
        "        if each[0][:-1] == ngram_1[i+1]:      \n",
        "#to find predictions based on highest probability of n-grams  \n",
        "                 \n",
        "            count +=1\n",
        "            pred_1[i+1].append(each[0][-1])\n",
        "            if count ==5:\n",
        "                break\n",
        "    if count<5:\n",
        "        while(count!=5):\n",
        "            pred_1[i+1].append(\"NOT FOUND\")           \n",
        "#if no word prediction is found, replace with NOT FOUND\n",
        "            count +=1\n",
        "for i in range(4):\n",
        "    ngrams_prob[i+1] = sorted(ngrams_prob[i+1], key = lambda x:x[1], reverse = True)\n",
        "    \n",
        "pred_2 = {1:[], 2:[], 3:[]}\n",
        "for i in range(3):\n",
        "    count = 0\n",
        "    for each in ngrams_prob[i+2]:\n",
        "        if each[0][:-1] == ngram_2[i+1]:\n",
        "            count +=1\n",
        "            pred_2[i+1].append(each[0][-1])\n",
        "            if count ==5:\n",
        "                break\n",
        "    if count<5:\n",
        "        while(count!=5):\n",
        "            pred_2[i+1].append(\"\\0\")\n",
        "            count +=1"
      ],
      "metadata": {
        "id": "hAl0J6ZYiYc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "use the n-grams of the strings to get the next word predictions based on the highest probabilities.\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "qRPvv3lOjYPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(4):\n",
        "    ngrams_prob[i+1] = sorted(ngrams_prob[i+1], key = lambda x:x[1], reverse = True)\n",
        "    \n",
        "pred_1 = {1:[], 2:[], 3:[]}\n",
        "for i in range(3):\n",
        "    count = 0\n",
        "    for each in ngrams_prob[i+2]:\n",
        "        if each[0][:-1] == ngram_1[i+1]:      \n",
        "#to find predictions based on highest probability of n-grams  \n",
        "                 \n",
        "            count +=1\n",
        "            pred_1[i+1].append(each[0][-1])\n",
        "            if count ==5:\n",
        "                break\n",
        "    if count<5:\n",
        "        while(count!=5):\n",
        "            pred_1[i+1].append(\"NOT FOUND\")           \n",
        "#if no word prediction is found, replace with NOT FOUND\n",
        "            count +=1\n",
        "for i in range(4):\n",
        "    ngrams_prob[i+1] = sorted(ngrams_prob[i+1], key = lambda x:x[1], reverse = True)\n",
        "    \n",
        "pred_2 = {1:[], 2:[], 3:[]}\n",
        "for i in range(3):\n",
        "    count = 0\n",
        "    for each in ngrams_prob[i+2]:\n",
        "        if each[0][:-1] == ngram_2[i+1]:\n",
        "            count +=1\n",
        "            pred_2[i+1].append(each[0][-1])\n",
        "            if count ==5:\n",
        "                break\n",
        "    if count<5:\n",
        "        while(count!=5):\n",
        "            pred_2[i+1].append(\"\\0\")\n",
        "            count +=1"
      ],
      "metadata": {
        "id": "ibPuGpSQjeNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Next word predictions for the strings using the probability models of bigrams, trigrams, and fourgrams\\n\")\n",
        "print(\"String 1 - after that alice said the-\\n\")\n",
        "print(\"Bigram model predictions: {}\\nTrigram model predictions: {}\\nFourgram model predictions: {}\\n\" .format(pred_1[1], pred_1[2], pred_1[3]))\n",
        "print(\"String 2 - alice felt so desperate that she was-\\n\")\n",
        "print(\"Bigram model predictions: {}\\nTrigram model predictions: {}\\nFourgram model predictions: {}\" .format(pred_2[1], pred_2[2], pred_2[3]))"
      ],
      "metadata": {
        "id": "vcE2n6cykT5w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}